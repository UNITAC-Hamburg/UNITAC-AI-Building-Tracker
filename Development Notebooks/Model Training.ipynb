{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJIhHLYw51HQ"
      },
      "source": [
        "# UNITAC eThekwini Building Tracker Model Training Workbook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhH3_u1-55wI"
      },
      "source": [
        "## 1. Imports and GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2uRbviT54r0"
      },
      "outputs": [],
      "source": [
        "# Hide output of this cell and install packages\n",
        "%%capture\n",
        "!pip install fastcore fastai --upgrade\n",
        "!pip3 install SemTorch\n",
        "\n",
        "# Import packages\n",
        "from fastai.vision.all import *\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import pytz\n",
        "from datetime import datetime\n",
        "from semtorch import get_segmentation_learner\n",
        "from google.colab import drive\n",
        "from IPython.display import Audio, display\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ni_20orH5_5e",
        "outputId": "60b06322-00d5-4914-920e-42344db77bff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue May 17 14:33:05 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check, which GPU was allocated \n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObNZm7TS6Cx9"
      },
      "source": [
        "## 2. Define Required Functions and Set Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbOnO1qg6MLU"
      },
      "source": [
        "### 2.1. Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjMmxUHT6DFF"
      },
      "outputs": [],
      "source": [
        "# Custom loss functions\n",
        "class CombinedLoss:\n",
        "    '''Dice and Focal combined'''\n",
        "    def __init__(self, axis=1, smooth=1., alpha=1.):\n",
        "        store_attr()\n",
        "        self.focal_loss = FocalLossFlat(axis=axis)\n",
        "        self.dice_loss =  DiceLoss(axis, smooth)\n",
        "        \n",
        "    def __call__(self, pred, targ):\n",
        "        return self.focal_loss(pred, targ) + self.alpha * self.dice_loss(pred, targ)\n",
        "    \n",
        "    def decodes(self, x):    return x.argmax(dim=self.axis)\n",
        "    def activation(self, x): return F.softmax(x, dim=self.axis)\n",
        "\n",
        "\n",
        "class Dual_Focal_loss(nn.Module):\n",
        "    '''\n",
        "    This loss is proposed in this paper: https://arxiv.org/abs/1909.11932\n",
        "    '''\n",
        "\n",
        "    def __init__(self, ignore_lb=255, eps=1e-5, reduction='mean'):\n",
        "        super(Dual_Focal_loss, self).__init__()\n",
        "        self.ignore_lb = ignore_lb\n",
        "        self.eps = eps\n",
        "        self.reduction = reduction\n",
        "        self.mse = nn.MSELoss(reduction='none')\n",
        "\n",
        "    def forward(self, logits, label):\n",
        "        ignore = label.data.cpu() == self.ignore_lb\n",
        "        n_valid = (ignore == 0).sum()\n",
        "        label = label.clone()\n",
        "        label[ignore] = 0\n",
        "        lb_one_hot = logits.data.clone().zero_().scatter_(1, label.unsqueeze(1), 1).detach()\n",
        "\n",
        "        pred = torch.softmax(logits, dim=1)\n",
        "        loss = -torch.log(self.eps + 1. - self.mse(pred, lb_one_hot)).sum(dim=1)\n",
        "        loss[ignore] = 0\n",
        "        if self.reduction == 'mean':\n",
        "            loss = loss.sum() / n_valid\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "        elif self.reduction == 'none':\n",
        "            loss = loss\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8uGTFtx6Ox4"
      },
      "source": [
        "### 2.2. Mount Google Drive, Set Path and Codes, and Define Required Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSLZp4W16R85",
        "outputId": "e6cc1457-6492-49bf-d529-f28aac869451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set path of root folder of images and masks\n",
        "path = Path(f'/content/drive/MyDrive/Segmentation Data/aerial')\n",
        "\n",
        "# Set codes\n",
        "codes = ['Background', 'Building']\n",
        "\n",
        "def n_codes(fnames, is_partial=True):\n",
        "  '''Gather the codes from a list of fnames'''\n",
        "  vals = set()\n",
        "  if is_partial:\n",
        "    random.shuffle(fnames)\n",
        "    fnames = fnames[:10]\n",
        "  for fname in fnames:\n",
        "    msk = np.array(PILMask.create(fname))\n",
        "    for val in np.unique(msk):\n",
        "      if val not in vals:\n",
        "        vals.add(val)\n",
        "  vals = list(vals)\n",
        "  p2c = dict()\n",
        "  for i,val in enumerate(vals):\n",
        "    p2c[i] = vals[i]\n",
        "  return p2c\n",
        "\n",
        "def get_msk(fn, p2c):\n",
        "  '''Grab a mask from a filename and adjust the pixels based on p2c'''\n",
        "  pix2class = n_codes(lbl_names)\n",
        "  # old structure: fn = f'{path}/buildings_mask_tiles/2019_10cm_RGB_BE_67/{tile_type}/{fn.stem[:-3]}lbl{fn.suffix}'\n",
        "  fn = str(fn).replace('image_tiles', 'buildings_mask_tiles')\n",
        "  msk = np.array(PILMask.create(fn))\n",
        "  mx = np.max(msk)\n",
        "  for i, val in enumerate(p2c):\n",
        "    msk[msk==p2c[i]] = val\n",
        "  return PILMask.create(msk)\n",
        "\n",
        "def get_msk_augmented(fn, p2c):\n",
        "  '''Grab a mask from a `filename` and adjust the pixels based on `pix2class`'''\n",
        "  fn = str(fn).replace('img', 'lbl')\n",
        "  msk = np.array(PILMask.create(fn))\n",
        "  mx = np.max(msk)\n",
        "  for i, val in enumerate(p2c):\n",
        "    msk[msk==p2c[i]] = val\n",
        "  return PILMask.create(msk)\n",
        "\n",
        "def get_y(o):\n",
        "  return get_msk(o, p2c)\n",
        "\n",
        "def get_y_augmented(o):\n",
        "  return get_msk_augmented(o, p2c)\n",
        "\n",
        "def batch_size(backbone, tile_size):\n",
        "  '''Automatically set batch size depending on image size and architecture used'''\n",
        "  if '512' in tile_size:\n",
        "    batch_size_dict = {'resnet152': 2, 'resnet101': 2, 'resnet50': 2, # Change batch size for used backbone if you run into CUDA out of memory errors\n",
        "                       'resnet34': 11, 'resnet18': 8, 'vgg16_bn': 2,\n",
        "                       'hrnet_w18': 32, 'hrnet_w30': 32, 'hrnet_w32': 32,\n",
        "                       'hrnet_w48': 18}\n",
        "  elif '256' in tile_size:\n",
        "    batch_size_dict = {'resnet152': 2, 'resnet101': 2, 'resnet50': 2,\n",
        "                       'resnet34': 11, 'resnet18': 10}\n",
        "  return batch_size_dict[backbone]\n",
        "\n",
        "def create_missing_folder(folder):\n",
        "  '''Create missing folders'''\n",
        "  if not os.path.exists(folder):\n",
        "    os.makedirs(folder)\n",
        "\n",
        "def timestamp():\n",
        "  '''Timestamp experiments'''\n",
        "  tz = pytz.timezone('Europe/Berlin')\n",
        "  date = str(datetime.now(tz)).split(\" \")\n",
        "  date_time = f\"{date[0]}_{date[1].split('.')[0][:5]}\"\n",
        "  return date_time\n",
        "\n",
        "def model_notification():\n",
        "  '''Create notification when model training is completed'''\n",
        "  for i in range(5):\n",
        "    display(Audio('https://www.soundjay.com/buttons/beep-03.wav', autoplay=True))\n",
        "    time.sleep(2)\n",
        "\n",
        "def get_tile_size(tile_type):\n",
        "  if '512' in tile_type:\n",
        "    tile_size = '512'\n",
        "  elif '256' in tile_type:\n",
        "    tile_size = '256'\n",
        "  return tile_size\n",
        "\n",
        "def check_fnames_lbls(tile_type, augmented=None):\n",
        "  '''Get images and labels for dataloader and check whether their number is equal'''\n",
        "  global fnames, lbl_names, path\n",
        "  if augmented == False:\n",
        "    path = Path(f'/content/drive/MyDrive/Segmentation Data/aerial') # change this to your data\n",
        "    # old structure: fnames = get_image_files(f'{path}/image_tiles/2019_10cm_RGB_BE_67/{tile_type}')\n",
        "    # old structure: lbl_names = get_image_files(f'{path}/buildings_mask_tiles/2019_10cm_RGB_BE_67/{tile_type}')\n",
        "    fnames = get_image_files(f'{path}/image_tiles')\n",
        "    lbl_names = get_image_files(f'{path}/buildings_mask_tiles')\n",
        "  elif augmented == True:\n",
        "    path = Path(f'/content/drive/MyDrive/Segmentation Data/aerial/augmented/8/0.2') # change this to your data\n",
        "    fnames = get_image_files(path/'img')\n",
        "    lbl_names = get_image_files(path/'lbl')\n",
        "  if len(fnames) != len(lbl_names):\n",
        "    print('ERROR: unequal number of image and mask tiles!')\n",
        "  return fnames, lbl_names, path\n",
        "\n",
        "def callbacks(model_dir, architecture, backbone, fit_type, timestamp):\n",
        "  '''Log results in CSV, show progress, and stop early if dice coefficient doesn't improve for 10 epochs'''\n",
        "  cbs = [CSVLogger(fname = f'{model_dir}/{architecture}_{backbone}_{fit_type}_{timestamp()}.csv', append=True),\n",
        "        ShowGraphCallback(),\n",
        "        EarlyStoppingCallback(monitor='dice', patience=10, reset_on_fit=True)]\n",
        "  return cbs\n",
        "\n",
        "def check_dataset_balance(tile_type, augmented=None):\n",
        "  '''Check, how balanced the dataset is'''\n",
        "  global tile_size, p2c\n",
        "  tile_size = get_tile_size(tile_type)\n",
        "\n",
        "  # Check if there is a label for each image\n",
        "  fnames, lbl_names, path = check_fnames_lbls(tile_type, augmented)\n",
        "\n",
        "  # Get codes of masks\n",
        "  p2c = n_codes(lbl_names)\n",
        "\n",
        "  if augmented == False:\n",
        "    label_func = get_y\n",
        "  elif augmented == True:\n",
        "    label_func = get_y_augmented\n",
        "\n",
        "  # Create dataloader to check building pixels\n",
        "  dls = SegmentationDataLoaders.from_label_func(path, fnames, label_func=label_func, bs=64, codes=codes, seed=2)\n",
        "\n",
        "  targs = torch.zeros((0, 512, 512))\n",
        "  for _, masks in dls[0]:\n",
        "    targs = torch.cat((targs, masks.cpu()), dim=0)\n",
        "\n",
        "  total_pixels = targs.shape[1]**2\n",
        "  percentages = torch.count_nonzero(targs, dim=(1, 2)) / total_pixels\n",
        "  plt.hist(percentages, bins=20)\n",
        "  plt.ylabel('Number of tiles')\n",
        "  plt.xlabel('Ratio of pixels that are of class `building`')\n",
        "  plt.gca().spines['top'].set_color('none')\n",
        "  plt.gca().spines['right'].set_color('none')\n",
        "  plt.show()\n",
        "  print(f'Mean Percentage of Pixels Belonging to Buildings: {round(percentages.mean().item(), 3)}')\n",
        "  return percentages\n",
        "\n",
        "def u_net_model_training(tile_type, backbone, fit_type, epochs, architecture='U-Net', augmented=None, split=.2):\n",
        "  '''Create list of files and masks, a dataloader, a model, callbacks, and train final model'''\n",
        "  global tile_size, p2c, loss\n",
        "  tile_size = get_tile_size(tile_type)\n",
        "  # Create additional image augmentations\n",
        "  tfms = [*aug_transforms(mult=1.0, do_flip=True, flip_vert=True, max_rotate=40.0, min_zoom=1.0, max_zoom=1.4, max_warp=0.4),\n",
        "   Normalize.from_stats(*imagenet_stats)]\n",
        "\n",
        "  # Check if there is a label for each image\n",
        "  fnames, lbl_names, path = check_fnames_lbls(tile_type, augmented)\n",
        "\n",
        "  # Get codes of masks\n",
        "  p2c = n_codes(lbl_names)\n",
        "\n",
        "  # Automatically set batch size depending on image size and backbone used\n",
        "  bs = batch_size(backbone, tile_size)\n",
        "\n",
        "  if augmented == False:\n",
        "    # Create function to load images and masks\n",
        "    dls = SegmentationDataLoaders.from_label_func(path, fnames, label_func=get_y, valid_pct=split, bs=bs, codes=codes, seed=2, batch_tfms=tfms)\n",
        "\n",
        "  elif augmented == True:\n",
        "    # Create custom splitting function to exclude images in the 'valid' folder from training\n",
        "    splitter = FuncSplitter(lambda fn: Path(fn).parent.name == 'valid')\n",
        "    db = DataBlock(blocks=(ImageBlock, MaskBlock(codes)), get_items=get_image_files, splitter=splitter, get_y=get_y_augmented, batch_tfms=tfms)\n",
        "    dls = db.dataloaders(path/'img', bs=bs, valid_pct=split)\n",
        "  \n",
        "  # Create model\n",
        "  if backbone == 'resnet18':\n",
        "    learn = unet_learner(dls, resnet18, n_out=2, loss_func=Dual_Focal_loss(), metrics=[Dice()] # Dice coefficient since dataset is imbalanced\n",
        "                        ).to_fp16() # 16-bits floats, which take half the space in RAM\n",
        "  elif backbone == 'resnet34':\n",
        "    learn = unet_learner(dls, resnet34, n_out=2, loss_func=Dual_Focal_loss(), metrics=[Dice()]).to_fp16()\n",
        "  elif backbone == 'resnet50':\n",
        "    learn = unet_learner(dls, resnet50, n_out=2, loss_func=Dual_Focal_loss(), metrics=[Dice()]).to_fp16()\n",
        "  elif backbone == 'resnet101':\n",
        "    learn = unet_learner(dls, resnet101, n_out=2, loss_func=Dual_Focal_loss(), metrics=[Dice()]).to_fp16()\n",
        "\n",
        "  learn.fit_one_cycle(epochs, cbs=callbacks(model_dir, architecture, backbone, fit_type, timestamp))\n",
        "  return learn, dls\n",
        "\n",
        "def seed():\n",
        "  # Create Seed for Reproducibility\n",
        "  number_of_the_seed = 2022\n",
        "  random.seed(number_of_the_seed)\n",
        "  set_seed(number_of_the_seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def hrnet_model_training(tile_type, backbone, fit_type, epochs, architecture='HRNet', augmented=None, split=.2, bs=None):\n",
        "  global tile_size, p2c, loss\n",
        "  seed()\n",
        "\n",
        "  tile_size = get_tile_size(tile_type)\n",
        "  fnames, lbl_names, path = check_fnames_lbls(tile_type, augmented)\n",
        "\n",
        "  if bs == None:\n",
        "    bs = batch_size(backbone, tile_size)\n",
        "\n",
        "  # Get codes of masks\n",
        "  p2c = n_codes(lbl_names)\n",
        "\n",
        "  if augmented == False:\n",
        "    # Create function to load images and masks\n",
        "    dls = SegmentationDataLoaders.from_label_func(path, fnames, label_func=get_y, bs=bs, codes=codes, seed=2022,\n",
        "                                                  batch_tfms=[Normalize.from_stats(*imagenet_stats)], valid_pct=split)\n",
        "  elif augmented == True:\n",
        "    splitter = FuncSplitter(lambda fn: Path(fn).parent.name == 'valid')\n",
        "    db = DataBlock(blocks=(ImageBlock, MaskBlock(codes)), get_items=get_image_files, splitter=splitter, get_y=get_y_augmented)\n",
        "    dls = db.dataloaders(path/'img', bs=bs, valid_pct=split)\n",
        "\n",
        "  learn = get_segmentation_learner(dls, number_classes=2, segmentation_type=\"Semantic Segmentation\", architecture_name=\"hrnet\",\n",
        "   backbone_name=backbone, model_dir=model_dir, metrics=[Dice()], splitter=trainable_params, pretrained=True).to_fp16()\n",
        "\n",
        "  learn.fit_one_cycle(epochs, cbs=callbacks(model_dir, architecture, backbone, fit_type, timestamp))\n",
        "  return learn, dls\n",
        "\n",
        "# Set path to model directory to store model and callbacks\n",
        "model_dir = f'{path}/models/Ferris'\n",
        "create_missing_folder(model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "WrOIRN-BT30p",
        "outputId": "b4750574-7286-4010-8ae5-89765027882c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1142: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  ret = func(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Percentage of Pixels Belonging to Buildings: 0.064\n"
          ]
        }
      ],
      "source": [
        "tile_type = '512_512 stride'\n",
        "\n",
        "percentages = check_dataset_balance(tile_type, augmented=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehHWlsnrimHW"
      },
      "source": [
        "### 3.1. Train U-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8LN8UJScA2B"
      },
      "source": [
        "## Unaugmented Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DD8hA_Pg8zi"
      },
      "outputs": [],
      "source": [
        "tile_type = '512_512 stride'\n",
        "backbone = 'resnet18'\n",
        "fit_type = 'One-Cycle'\n",
        "epochs = 200\n",
        "\n",
        "learn, dls = u_net_model_training(tile_type, backbone, fit_type, epochs, architecture='U-Net', augmented=False, split=.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCZ5oppjb8X6"
      },
      "source": [
        "## Augmented Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw4zHGyhb6AF"
      },
      "outputs": [],
      "source": [
        "tile_type = '512_512 stride'\n",
        "backbone = 'resnet18'\n",
        "fit_type = 'One-Cycle'\n",
        "epochs = 200\n",
        "\n",
        "learn, dls = u_net_model_training(tile_type, backbone, fit_type, epochs, architecture='U-Net', augmented=True, split=.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh3fGOr16O0d"
      },
      "source": [
        "### 3.2. Train HRNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1HmbOvlcTHH"
      },
      "source": [
        "##Unaugmented Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3u0ajEUilYb"
      },
      "outputs": [],
      "source": [
        "tile_type = '512_512 stride'\n",
        "backbone = 'hrnet_w18'\n",
        "fit_type = 'One-Cycle'\n",
        "epochs = 200\n",
        "\n",
        "learn, dls = hrnet_model_training(tile_type, backbone, fit_type, epochs, architecture='HRNet', augmented=False, split=.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2zaLP6tcYWx"
      },
      "source": [
        "## Augmented Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHcJdyIDcc0m"
      },
      "outputs": [],
      "source": [
        "tile_type = '512_512 stride'\n",
        "backbone = 'hrnet_w18'\n",
        "fit_type = 'One-Cycle'\n",
        "epochs = 200\n",
        "\n",
        "learn, dls = hrnet_model_training(tile_type, backbone, fit_type, epochs, architecture='HRNet', augmented=True, split=.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHnqMtIdflHN"
      },
      "source": [
        "## Save models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plplHpQGfl_5"
      },
      "outputs": [],
      "source": [
        "model_save_path = 'drive/newmodel' # put path to where you want to save the model here\n",
        "learn.export(f'{model_save_path}_exported.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtKeGSncf93s"
      },
      "source": [
        "For information on functions and everything related to fastai, which was used to train the models, go to https://docs.fast.ai/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "UNITAC eThekwini Building Tracker Model Training Workbook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
